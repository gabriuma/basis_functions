
\documentclass[onecolumn,a4paper,11pt]{article}
\voffset -2cm
\hoffset -1cm
\textheight 23.3cm
\textwidth 15.0cm

\usepackage{blindtext}
%\usepackage{titlesec}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{microtype}

\usepackage{lscape}

\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

\usepackage{epstopdf}

\usepackage{natbib}
\bibliographystyle{plainnat}
\bibpunct{(}{)}{;}{a}{,}{,}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{float}
\usepackage{soul}
\usepackage{xcolor}
\graphicspath{{./images/}}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{url}
\usepackage{parskip}

\usepackage[titletoc]{appendix}
\usepackage{booktabs}
\usepackage{boldline}
\usepackage{colortbl}
\DeclareMathOperator{\LogNormal}{LogNormal}
\DeclareMathOperator{\GP}{\mathcal{GP}}
\DeclareMathOperator{\Normal}{Normal}

\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}


% \bibpunct[, ]{[}{]}{,}{}{,}{,}
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}

\usepackage{listings}
\usepackage{slashbox}
\usepackage{caption}
\captionsetup{font=footnotesize}
\usepackage[colorlinks=true,citecolor=black,linkcolor=black,urlcolor=black,breaklinks=true]{hyperref}

%\bibliographystyle{authordate1}
% TODO macro
\newcommand{\todo}[1]{\textcolor{blue}{\textbf{[#1]}}}
\newcommand{\resp}[1]{\textcolor{red}{\textbf{[#1]}}}


\title{\begin{flushleft}
\textsf{\small Statistics and Computing}
\end{flushleft}  \vspace{0.5cm}  Supplementary material for "Practical Hilbert space approximate Bayesian Gaussian processes for probabilistic programming"}

\author{Gabriel Riutort-Mayol$^{1*}$, Paul-Christian BÃ¼rkner$^2$, Michael R.\ Andersen$^{3}$,\\
  Arno Solin$^{4}$, Aki Vehtari$^{4}$}

\date{ \small
$^1$ Department of Cartographic Engineering, Geodesy, and Photogrammetry, Universitat Polit\`ecnica de Val\`encia, Spain 
\break
$^2$ Excellence Cluster for Simulation Technology, University of Stuttgart, Germany
\break
$^3$ Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark
\break
$^4$ Department of Computer Science, Aalto University, Finland
\break
$^*$ Corresponding author, Email: gabriuma@gmail.com
}


\begin{document}

\maketitle

This supplementary material consists of four case studies of application of the methodology proposed in the article.

\tableofcontents

\appendix

\counterwithin{figure}{section}
\counterwithin{table}{section}
\counterwithin{equation}{section}

\section{Same-sex marriage case study}\label{ch5_sec_studycaseII}
This data set relates the proportion of support for same-sex marriage to the age. The data consists of 74 observations of the amount of people $y_i$ supporting same-sex marriage from a population $n_i$ per age group $i$ ($i=1,\dots,74$). The observational model is a binomial model with parameters population $n_i$ and probability of supporting same-sex marriage $p_i$ per age group $i$,
%
\begin{equation*}
y_i \sim \mathrm{Binomial}(p_i, n_i).
\end{equation*}

\noindent The population per age group $n_i$ is a known quantity and the goal is to estimate the same-sex support probability $p_i$ or mean number of support people per age group. Probabilities $\bm{p}=(p_1,\dots,p_{74})$ are modeled by a Gaussian process (GP) function $f:{\rm I\!R} \to {\rm I\!R}$ with a squared exponential covariance function $k$, as a function of age input values $\bm{x}=(x_1,\dots,x_{74})$, and through the {\it logit} function as a link function:
%
\begin{align*} \label{ch5_eq_gpprior_gay}
p_i &= \mathrm{logit}(f(x_i)) \nonumber \\
f(x) &\sim \mathcal{GP}(0, k(x,x', \theta)).
\end{align*}

\noindent Saying that the function $f(\cdot)$ follows a GP model is equivalent to say that $\bm{f}$ is multivariate Gaussian distributed with covariance matrix $\bm{K}$, where $K_{ij}=k(x_i,x_j,\theta)$, with $i,j=1,\dots,74$. The covariance function $k$ depends on the inputs $\bm{x}$ and hyperparameters $\theta=\{\alpha,\ell\}$. The hyperparameters $\alpha$ and $\ell$ represent the marginal variance and length-scale, respectively, of the GP process.

In the HSGP model, the function $f(x)$ is approximated as in equation (7)%(\ref{eq_approxf})
, with the squared exponential spectral density as in equation (1)%(\ref{eq_specdens_inf})
, and eigenvalues $\lambda_j$  and eigenfunctions $\phi_j$ as in equations (5) %(\ref{eq_eigenvalue}) 
and (6)%(\ref{eq_eigenfunction})
. 

In order to do model comparison, in addition to the exact GP and HSGP models, an spline-based model is also fitted using the Thin Plate Regression Splines approach in \cite{wood2003thin} and implemented in the R-package \textit{mgcv} \citep{wood2011mgcv}. A Bayesian approach is used to fit this spline-based model using the R-package \textit{brms} \citep{burkner2017brms}.

Figure \ref{ch5_fig12_Posteriors_gaydata} shows the posterior mean predictive distributions of the three models, the exact GP, the HSGP with $m=20$ basis functions and boundary factor $c=1.5$, and the spline-based model with 20 knots. Sample observations are plotted as circles in the figure, and the out-of-sample (test) observations, which have been used for testing, are plotted as crosses.

\begin{figure}
\centering
\includegraphics[scale=0.50]{ch5_fig12_Posteriors_gaydata.pdf}
\caption{Posterior predictive means of the proposed HSGP, the exact GP, and the spline-based models. 95\% confident intervals are plotted as dashed lines.}
  \label{ch5_fig12_Posteriors_gaydata}
\end{figure}

For the HSGP, different models with different number of basis functions and boundary factor have been fitted. The root mean square errors (RMSE) for every one of these models have been computed against the exact GP, and plotted as a function of the number of basis functions $m$ and boundary factor $c$ in Figure \ref{ch5_fig13_MSE_train_BF_gaydata}, for sample (left) and test (right) data. The expected patterns of the approximation as a function of the number of basis functions and boundary factor are recognized: as the boundary factor increases, more basis functions are needed.

Figure \ref{ch5_fig14_MSE_train_gaydata} shows the RMSE of the exact GP, HSGP and spline-based models computed against the actual data, for training and test data, as a function of the number of basis functions $m$ and boundary factor $c$, for the HSGP, and knots for the spline-based model. We can see how the spline-based model does not extrapolate data properly (Figure \ref{ch5_fig14_MSE_train_gaydata}-right). 

Figure \ref{ch5_fig14_1_time_gaydata} shows computational times, in seconds per iteration (iteration of the HMC sampling method), as a function of the number of basis functions $m$, for the HSGP, and knots, for the spline-based model. The computational times is represented in the y-axis which is on a logarithmic scale. The HSGP is on average roughly 15 times faster than the exact GP and 5 times faster than the spline-based model, for this particular case and univariate input space. It can be appreciated that the computational time increases relatively slightly with the number of basis functions in a univariate input space.

The Stan model codes for the exact GP, the HSGP and the spline-based models of this case study can be found online at {\small \url{https://github.com/gabriuma/basis_functions_approach_to_GP/tree/master/Paper/Case-study_Same-sex-marriage-data}}\,.

%\begin{lstlisting}[breaklines]
%https://github.com/gabriuma/basis_functions_approach_to_GP/tree/master/Paper/Case-study_Same-sex-marriage-data
%\end{lstlisting}

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig13_MSE_train_BF_gaydata.pdf}}
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig13_MSE_pred_BF_gaydata.pdf}}
\subfigure{\includegraphics[scale=0.40, trim = 28mm 45mm 119mm 0mm, clip]{ch5_fig13_legend_gaydata.pdf}}
\caption{Root mean square error (RMSE) of the HSGP, computed against the exact GP, as a function of the number of basis functions $m$ and boundary factor $c$. RMSE for sample (left) and test (right) data.}
  \label{ch5_fig13_MSE_train_BF_gaydata}
\end{figure}

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig14_MSE_train_gaydata.pdf}}
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig14_MSE_pred_gaydata.pdf}}
\subfigure{\includegraphics[scale=0.40, trim = 27mm 35mm 112mm 0mm, clip]{ch5_fig14_legend_gaydata.pdf}}
\caption{Root mean square error (RMSE) of the different methods, exact GP, HSGP and spline-based models, computed against the actual data, as a function of the number of basis functions $m$ and boundary factor $c$, for the HSGP, and knots, for the spline-based model. RMSE for sample (left) and test (right) data.}
  \label{ch5_fig14_MSE_train_gaydata}
\end{figure}

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig14_1_time_gaydata.pdf}}
\caption{Computational time (y-axis), in seconds per iteration (iteration of the HMC sampling method), as a function of the number of basis functions $m$, for the HSGP, and knots, for the spline-based model. The y-axis is in a logarithmic scale.}
  \label{ch5_fig14_1_time_gaydata}
\end{figure}


\section{Case study of simulated data for a bivariate function}

This example consists of a synthetic dataset with $n=120$ ($i=1,\dots,n$) single draws from a GP prior as a function of input values $\bm{x}_i \in \{[-1,1],[-1,1]\} \subset {\rm I\!R}^2$ in a bivariate input space ($D=2$). A squared exponential covariance function, with hyperparameters marginal variance $\alpha=1$ and length-scales $\ell_1=0.10$, for the first input dimension, and $\ell_2=0.35$, for the second input dimension, is used for the GP prior. Gaussian noise standard deviation $\sigma=0.2$ was added to the GP draws to form the final noisy set of observations $\bm{y} \in {\rm I\!R}^{120}$.

The exact GP model over the outcome variable $\bm{y}$ can be written as follows,
%
\begin{eqnarray*}\label{ch5_eq_latentgp_simudata2}
\begin{split}
\bm{y} &= \bm{f} + \bm{\epsilon} \\
\bm{\epsilon} &\sim \Normal(0, \sigma^2  I) \\
f(\bm{x}) &\sim \mathcal{GP}(0, k(\bm{x}, \bm{x}', \theta)),
\end{split}
\end{eqnarray*}

\noindent where $\bm{f}=\{f(\bm{x}_i)\}_{i=1}^{120}$ represents the underlying function at the input values $\bm{x_i}\in {\rm I\!R}^{2}$, $\bm{\epsilon}$ is the Gaussian noise term with variance $\sigma^2$, and $I$ represents the identity matrix. The function $f:{\rm I\!R}^2 \to {\rm I\!R}$ is a GP prior with a multivariate squared exponential covariance function $k$, which depends on the inputs $\bm{x}$ and hyperparameters $\theta=\{\alpha,\ell_1,\ell_2\}$. The hyperparameters $\alpha$, $\ell_1$ and $\ell_2$ represent the marginal variance and length-scales for first and second input dimensions, respectively, of the GP process. Saying that the function $f(\cdot)$ follows a GP model is equivalent to say that $\bm{f}$ is multivariate Gaussian distributed with covariance matrix $\bm{K}$, where $K_{ij}=k(\bm{x}_i,\bm{x}_j,\theta)$, with $i,j=1,\dots,120$.

The marginalized form, by integrating out the latent values $\bm{f}$, of the previous latent GP model results:
%
\begin{equation*}\label{ch5_eq_marginalizedgp_simudata2}
\bm{y} \sim \Normal(0, \bm{K} + \sigma^2 I ),
\end{equation*}
%
where $\bm{K} \in {\rm I\!R}^{n\times n}$ if the covariance matrix with an element $K_{ij}=k(\bm{x}_i,\bm{x}_j,\theta)$, and $\sigma^2$ is the noise variance.

In the HSGP model with $2$ input dimensions, the latent function $f(\bm{x})$ is approximated as in equation (13)% (\ref{eq_approxf_multi})
, with the $2$D squared exponential spectral density $S$ as in equation (1)% (\ref{eq_specdens_inf})
, and the $2$-vector of eigenvalues $\bm{\lambda}_j$ and the multivariate eigenfunctions $\phi_j$ as in equations (10) %(\ref{eq_eigenfunction_multi})
 and (11)% (\ref{eq_eigenvalue_multi})
 , respectively.


In order to do model comparison, in addition to the exact GP and HSGP models, a $2$D splines-based model is also fitted using a cubic spline basis, penalized by the conventional integrated square second derivative cubic spline penalty \citep{wood2017generalized}, and implemented in the R-package \textit{mgcv} \citep{wood2011mgcv}. A Bayesian approach is used to fit this spline-based model using the R-package \textit{brms} \citep{burkner2017brms}.

Figure \ref{ch5_fig15_Posteriors_exII} shows the data-generating GP function, from where the dataset was drawn, and the mean posterior predictive functions of the three models, exact GP, HSGP, and splines, fitted over the dataset. Sample observations are also plotted as circles. For the HSGP, $m_1=40$ and $m_2=40$ basis functions for each dimension, respectively, were used, which lead to a total of 1600 multivariate basis functions. A boundary factor for each dimension, $c_1=1.5$ and $c_2=1.5$, were used. For the spline-based model, 40 knots in each dimension were used.

Figure \ref{ch5_fig16_errors_exII} shows the difference functions between the data-generating function and the GP, HSGP and spline-based models, respectively.

\begin{figure}
\begin{center}
\begin{tabular}{ c c c}
\includegraphics[scale=0.40, trim = 0mm 33mm 35mm 20mm, clip]{ch5_fig15_truefun_exII.pdf} & \hspace{-5mm} \includegraphics[scale=0.40, trim = 19mm 33mm 35mm 20mm, clip]{ch5_fig15_gpfun_exII.pdf} & \hspace{-5mm}\multirow{-5.5}{*}{ \includegraphics[scale=0.35, trim = 150mm 18mm 0mm 10mm, clip]{ch5_fig15_colorbar_vertical_exII.pdf}}\\ 
\includegraphics[scale=0.40, trim = 0mm 18mm 35mm 20mm, clip]{ch5_fig15_bffun_exII.pdf}  & \hspace{-5mm} \includegraphics[scale=0.40, trim = 19mm 18mm 35mm 20mm, clip]{ch5_fig15_spfun_exII.pdf}
\end{tabular}
\end{center}
\caption{Data-generating function (a) and posterior predictive mean functions of the GP (b), HSGP (c) and spline-based (d) models. Sample points are plotted as circles}
  \label{ch5_fig15_Posteriors_exII}
\end{figure}


\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.36, trim = 1mm 18mm 39.2mm 20mm, clip]{ch5_fig16_errorGP_exII.pdf}}
\subfigure{\includegraphics[scale=0.36, trim = 19mm 18mm 39.2mm 20mm, clip]{ch5_fig16_errorHSGP_exII.pdf}}
\subfigure{\includegraphics[scale=0.36, trim = 19mm 18mm 39.2mm 20mm, clip]{ch5_fig16_errorSP_exII.pdf}}
\subfigure{\includegraphics[scale=0.30, trim = 150mm 5mm 0mm 20mm, clip]{ch5_fig16_error_colorbar_exII.pdf}}
\caption{Error between the data-generating function and posterior mean functions of the GP (a), HSGP (b) and spline-based (c) models. Sample points are plotted as circles.}
  \label{ch5_fig16_errors_exII}
\end{figure}

In order to assess performance of the models as a function of the number of basis functions, boundary factor and knots, different models with different number of basis functions, for the HSGP, and different number of knots, for the spline-based model, have been fitted. In each of these models, the same boundary factor, number of basis functions and knots per dimension were used. Figure \ref{ch5_fig17_RMSE_exII}-left shows the root mean squared error (RMSE), computed against the data-generating function, as a function of the boundary factor $c$ and number of univariate basis functions $m$, for the HSGP, and knots, for the spline-based model. From Figures \ref{ch5_fig16_errors_exII} and \ref{ch5_fig17_RMSE_exII}-left, it can be seen an accurate approximation of the HSGP to the exact GP. However, the performance of the spline-based model is significantly worse. 

Figure \ref{ch5_fig17_RMSE_exII}-right shows the computation times of the different models as a function of the boundary factor, number of basis functions and knots. Figure \ref{ch5_fig17_RMSE_exII} reveals that choosing the optimal boundary factor allows for less number of basis functions and less computation time. Even though in a bivariate input space the computational demands increase significantly with the number of dimensions or knots, either the HSGP or spline-based models work significantly better than exact GP, even for highly wiggly functions that require a high number of basis functions or knots for an accurate approximation. It is interesting to be noticed that just the computation of the input data for the spline model in a 2D input space and considering more than 50 knots per dimension is computationally very expensive.

The Stan model codes for the exact GP, the HSGP and the spline-based models of this case study can be found online at {\small \url{https://github.com/gabriuma/basis_functions_approach_to_GP/tree/master/Paper/Case-study_2D-Simulated-data}}\,.


\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig17_RMSE_exII.pdf}}
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig17_time_exII.pdf}}
\subfigure{\includegraphics[scale=0.40, trim = 28mm 35mm 112mm 0mm, clip]{ch5_fig17_legend_exII.pdf}}
\caption{Root mean square error (RMSE) (left) and computational time (right) in seconds per iteration (iteration of the HMC sampling method) of the different methods, and plotted as a function of the boundary factor $c$, number of basis functions $m$ and knots.}
  \label{ch5_fig17_RMSE_exII}
\end{figure}


\section{Diabetes case study}\label{ch5_sec_bf_caseV}
The next example presents an epidemiological study of diabetes disease. The study aims to relate the probability of suffering from diabetes to some risk factors. The data contains $n=392$ individuals ($i=1,\dots,n$) from which the binary variable of suffering ($y_i=1$) or not suffering ($y_i=0$) from diabetes have been observed. The input vector $\bm{x}_i=(x_{i1},x_{i2},x_{i3},x_{i4}) \in {\rm I\!R}^{4}$, in a 4D input space ($D=4$), contains the risk factors: \textit{Glucose} ($x_{i1}$), \textit{Pregnancy} ($x_{i2}$), \textit{Age} ($x_{i3}$) and \textit{BMI} ($x_{i4}$), per individual $i$. The observational model is a Bernoulli model with parameter the probability $p_i$ of suffering from diabetes per observation $i$,
%
\begin{equation*}
y_i \sim \mathrm{Bernoulli}(p_i).
\end{equation*}

\noindent The goal is to estimate the probability $p_i$ as a function ($f$) of the risk factors. Which function $f(\cdot):{\rm I\!R}^{4} \to {\rm I\!R}$ is modeled as a GP with a multivariate squared exponential covariance function $k$ depending on the risk factors $\bm{x}$ and hyperparameters $\theta=\{\alpha,\ell\}$, and related to the probabilities $p_i$ through the {\it logit} link function,
%
\begin{align*} \label{ch5_eq_gpprior_gay}
p_i &= \mathrm{logit}(f(\bm{x}_i)) \nonumber \\
f(\bm{x}) &\sim \mathcal{GP}(0, k(\bm{x},\bm{x}', \theta).
\end{align*}

\noindent The hyperparameters $\alpha$ and $\ell$ represent the marginal variance and length-scale, respectively, of the GP process. A scalar length-scale has been considered in the multivariate covariance function $k$.

In the HSGP model with $D$ input dimensions, the function $f(\bm{x})$ evaluated at input vector $\bm{x} \in {\rm I\!R}^D$ is approximated as in equation (13)% (\ref{eq_approxf_multi})
, with the $D$-dimensional (with a scalar length-scale) squared exponential spectral density $S$ as in equation (1) %(\ref{eq_specdens_inf}) 
and the $D$-vector of eigenvalues $\bm{\lambda}_j$ and the multivariate eigenfunctions $\phi_j$ as in equations (10) % (\ref{eq_eigenvalue_multi}) 
and (11)% (\ref{eq_eigenfunction_multi})
, respectively.


In order to do model comparison, in addition to the exact GP and HSGP models, a $D$-dimensional spline-based model is also fitted using a cubic spline basis penalized by the conventional integrated square second derivative cubic spline penalty \citep{wood2017generalized} and implemented in the R-package \textit{mgcv} \citep{wood2011mgcv}. A Bayesian approach is used to fit this spline-based model using the R-package \textit{brms} \citep{burkner2017brms}.

Figure \ref{ch5_fig18_gpfun_diabetes} shows the posterior mean predictions of probabilities ($p_i$) of the three models, exact GP, HSGP and splines, fitted over the dataset with the two input dimensions \textit{Glucose} and \textit{Pregnancy} ($D=2$). The binary observations $y_i$ are also plotted in the plots as colored points. For the HSGP, $m_1=20$ and $m_2=20$ basis functions for each dimension, respectively, were used, which lead to a total of 400 multivariate basis functions. A boundary factor for each dimension, $c_1=4$ and $c_2=4$, were used. For the spline-based model, 20 knots per dimension were used.
%
\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.37, trim = 5mm 23mm 39mm 20mm, clip]{ch5_fig18_gpfun_diabetes.pdf}}
\subfigure{\includegraphics[scale=0.37, trim = 20mm 23mm 39mm 20mm, clip]{ch5_fig18_bffun_diabetes.pdf}}
\subfigure{\includegraphics[scale=0.37, trim = 20mm 23mm 39mm 20mm, clip]{ch5_fig18_spfun_diabetes.pdf}}
\subfigure{\includegraphics[scale=0.30, trim = 150mm 4mm 2mm 15mm, clip]{ch5_fig18_legend_diabetes.pdf}}
\caption{Posterior predictive mean functions of the GP (a), HSGP (b) and spline-based (c) models. Sample observations of suffering (red points) and not suffering (blue points) from the disease are plotted.}
  \label{ch5_fig18_gpfun_diabetes}
\end{figure}

In order to assess the performance of the models as a function of the boundary factor, the number of basis functions and knots, different models with different number of basis functions and boundary factors, for the HSGP, and different number of knots, for the spline-based model, have been fitted. In each of these models, the same boundary factor, number of basis functions and knots per dimension were used. Figure \ref{ch5_fig19_ELPD_diabetes} shows the expected log predictive density (ELPD; see \cite{vehtari_2012}) as a function of the boundary factor $c$ and number of univariate basis functions $m$, for the HSGP, and knots, for the spline-based model. The ELPD is computed over the actual observations by cross-validation. With slightly differences, all models show similar performances due to the fact that the process is very smooth with a relatively very large length-scale estimate $\ell=4.51$. Even though, a slight improvement can be appreciated with larger boundary factor $c$, since small boundary factors are not allowed when large length-scales (Figure 6% \ref{fig6_lscale_vs_J_vs_c}
).
%
\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig19_ELPD_diabetes.pdf}}
\caption{Expected log predictive density (ELPD) of the different methods as a function of the boundary factor $c$ and the number of basis functions $m$, for the HSGP model, and knots, for the spline-based model.}
  \label{ch5_fig19_ELPD_diabetes}
\end{figure}

Figure \ref{ch5_fig20_time2D_diabetes} shows the computational times of the different models, exact GP, HSGP and splines, fitted over the dataset with 2, 3 and 4 input dimensions, as a function of the boundary factor $c$ and number of univariate basis functions $m$ and knots. We can appreciate the significant increase
of computational time with higher dimensions for the HSGP and spline models.
This fact reveals that choosing optimal values for the number of basis functions and
boundary factor, looking at the recommendations and diagnosis provided by Figure 6% \ref{fig6_lscale_vs_J_vs_c}
, is essential to avoid a excessive computational time, especially in high input
dimensionality. It is interesting to be noticed that considering the spline model in a 4D input space is not allowed for an amount of 392
observations. Similarly, just the computation of the input data for the spline model
in a 3D input space and considering more than 10 knots per dimension is computationally very expensive.

The Stan model codes for the exact GP, the HSGP and the spline-based models of this case study can be found online at {\small \url{https://github.com/gabriuma/basis_functions_approach_to_GP/tree/master/Paper/Case-study_Diabetes-data}}\,.

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.29, trim = 1mm 0mm 10mm 0mm, clip]{ch5_fig20_time2D_diabetes_2.pdf}}
\subfigure{\includegraphics[scale=0.29, trim = 30.2mm 0mm 10mm 0mm, clip]{ch5_fig20_time3D_diabetes_2.pdf}}
\subfigure{\includegraphics[scale=0.29, trim = 30.2mm 0mm 10mm 0mm, clip]{ch5_fig20_time4D_diabetes_2.pdf}}
\subfigure{\includegraphics[scale=0.35, trim = 28mm 50mm 112mm 10mm, clip]{ch5_fig20_legend_diabetes_2.pdf}}
\caption{Time of computation in seconds per iteration (iteration of the HMC sampling method) of the different models fitted over the dataset, with 2 (left), 3 (center) and 4 (right) input dimensions, as a function of the boundary factor $c$ and number of basis functions $m$, for the HSGP model, and knots, for the spline model. The y-axis is on a logarithmic scale.}
  \label{ch5_fig20_time2D_diabetes}
\end{figure}


\section{Spatio-temporal land-use classification case study}\label{ch5_sec_application_caseII}


The next example presents an spatio-temporal classification in land-use of plots between 2006 and 2015 in a part of the territory of Valencia in Spain dedicated to growing citrus fruits. A sampling set consists of $n=200$ plots with known class. The data is recorded in a time series of $T=5$ years (2006, 2008, 2010, 2012, 2015) within the period. The class of each parcel $i$ and time $t$ is stored by a categorical variable $y_{it}$ representing the $K=5$ different possible classes ($k=1,\dots,K$): $k=1$, adult independent citrus fruits; $k=2$, aligned citrus fruits; $k=3$, irregular citrus fruits; $k=4$, abandoned citrus fruits; $k=5$, young citrus fruits. 

A bunch of 52 characteristic variables was available for every parcel and time. These variables were computed from satellite color images and cadastral map by using the software FETEX for automatic descriptive feature extraction from image-objects \citep{ruiz2011feature}. These variables concern spectral intensities and empirical semivariogram of the pixels within a plot, as well as descriptive statistics of the shape of the plots.

Due to the fact that 52 input variables are too high-dimensional for a multivariate HSGP model, since computation scales as $O(n m^D+m^D)$ for $m$ basis functions and $D$ input variables, the multivariate HSGP model will be formulated as an additive HSGP model, where computational complexity is linear with the number of additive components. 

As it is known, the computational demand of a multivariate HSGP model component increases quickly with the number of dimensions, so we should avoid high-dimensional HSGP components in the additive model. Original input variables are highly correlated, which would imply the use of high-order interaction components in the additive model. Therefore, instead of using the original variables as inputs, we use their principal components (PCs), which are expect to be linearly uncorrelated. %Using the principal components as inputs helps, in principle, not to have to use as many high-order interaction components in the additive model.  

The PCs will be used jointly with the time variable as inputs in the classifying additive HSGP model. Let's denote the matrix $\bm{X}=[\bm{x}_{11}\, \cdots \, \bm{x}_{it} \, \cdots \, \bm{x}_{nT}]^\top \in {\rm I\!R}^{nT\times D}$, which contains the input vectors $\bm{x}_{it} \in {\rm I\!R}^D$, $D=53$ ($52 \text{ PCs plus time}$), for the spatio-temporal observations (plots $i=1,\dots,200$, and times $t=1,\dots,5$). 

The observational model is a multinomial model with parameters the vector of probabilities $\bm{p}_{it}=(p_{it,1},p_{it,2},\dots,p_{it,K})$, where $p_{it,k}$ is the probability of belonging to class $k$ per parcel $i$ and time $t$,
%
\begin{equation*}
y_{it} \sim \mathrm{Multinomial}(\bm{p}_{it}).
\end{equation*}

\noindent The goal is to estimate the vector of probabilities $\bm{p}_{it}$ as a function of the predictors, which is a multivariate function $f(\bm{x}_{it}):{\rm I\!R}^{D} \rightarrow {\rm I\!R}^{K}$,
$$
f(\bm{x}_{it})=\big(f_1(\bm{x}_{it}),\dots,f_K(\bm{x}_{it}) \big).
$$

\noindent $f(\bm{x}_{it})$ is related to the vector of probabilities $\bm{p}_{it}$ through the 'softmax' link function,
%
\begin{equation*}
\bm{p}_{it} = \mathrm{softmax}(f(\bm{x}_{it})).
\end{equation*}

Each individual function $f_k(\bm{x})$ is modeled as a first-order additive model plus the second-order additive effects between time input variable ($x^D$) and all the other inputs as follows: 
%
\begin{equation} \label{ch5_eq_f_parcels}
f_k(\bm{x}) = \sum_{d=1}^{D} g_d(x^d) + \sum_{d=1}^{D-1} h_{d,D}(x^d,x^D).
%\mathcal{HSGP}(x^d, S(\bm{\lambda}), \theta_d),
\end{equation}

\noindent The first-order components $\{g_d(x^d)\}_{d=1}^D$ in equation (\ref{ch5_eq_f_parcels}) are modeled as unidimensional HSGP models:
%
\begin{equation*}
g_d(x^d) \sim \mathcal{HSGP}(x^d, S, \theta_{d,1}).
\end{equation*}

\noindent In the HSGP model, a first-order component $g_d(x^d)$, evaluated at input value $x^d \in {\rm I\!R}$, is approximated as in equation (7), % (\ref{eq_approxf}) 
with the squared exponential spectral density $S$ as in equation (1) % (\ref{eq_specdens_inf})
 and eigenvalues $\lambda_j$  and eigenfunctions $\phi_j$ as in equations (5) % (\ref{eq_eigenvalue})
and (6)% (\ref{eq_eigenfunction})
  , respectively. 

The second-order components $\{h_{d,D}(x^d,x^D)\}_{d=1}^{D-1}$ in equation (\ref{ch5_eq_f_parcels}) are modeled as two-dimensional HSGP models:
%
\begin{equation*}
h_{d,D}(x^d,x^D) \sim \mathcal{HSGP}(x^d,x^D, S, \theta_{d,D}).
\end{equation*} 

\noindent In the HSGP model, a second-order component $h_{d,D}(x^d,x^D)$, evaluated at inputs $x^d \in {\rm I\!R}$ and $x^D \in {\rm I\!R}$, is approximated as in equation (13), % (\ref{eq_approxf_multi})
 with the two-dimensional (with a scalar length-scale) squared exponential spectral density $S$ as in equation (1) % (\ref{eq_specdens_inf}) 
 and the $D$-vector of eigenvalues $\bm{\lambda}_j$ and multivariate eigenfunctions $\phi_j$ as in equations (10) % (\ref{eq_eigenvalue_multi}) 
 and (11)% (\ref{eq_eigenfunction_multi})
 , respectively. 

The vector of hyperparameters $\theta_{d,1}=(\alpha_{d,1},\ell_{d,1})$ contains the marginal variance $\alpha_{d,1}$ and length-scale $\ell_{d,1}$ of the $g_d(x^d)$ model component. And, the vector of hyperparameters $\theta_{d,D}=(\alpha_{d,D},\ell_{d,D})$ contains the marginal variance $\alpha_{d,D}$ and length-scale $\ell_{d,D}$ of the $h_{d,D}(x^d,x^D)$ model component.
 

For the first-order components $g_d(x^d)$, $m=15$ basis functions and a boundary factor $c=2.5$ were used. For the second-order components $h_{d,D}(x^d,x^D)$, $m_1=15$ and $m_2=15$ basis functions for each dimension, respectively, were used, which lead to a total of 225 multivariate basis functions. A boundary factor for each dimension, $c_1=2.5$ and $c_2=2.5$, respectively, were used. All the input variables were previously standardised.

In the case of the first-order components, the normalized length-scale estimates $\left(\frac{2\cdot \hat{\ell}_{d,1}}{|x^d_{max}-x^d_{min}|}\right)$ are all bigger than the minimum length-scale reported by Figure (6) % \ref{fig6_lscale_vs_J_vs_c} 
as a function of $m$ and $c$. Which means that the used number of basis functions ($m=15$) and boundary factor ($c=2.5$) are suitable values for modeling accurately the input effects.

For the second-order components, the relationships between the number of basis functions, the boundary factor and the length-scale is not available for the multivariate case. However, we can approximately diagnose the length-scale estimates of the second-order HSGP components analyzing each dimension separately as unidimensional HSGP models.

Table \ref{ch5_tab_parcels} shows the confusion matrix after fitting the model following a $Q$-fold cross-validation procedure, with $Q=100$, over the training data. Thus, every fold contains $10$ observations. The confusion matrix evaluates the rate of misclassification per class. Columns represent the true classes and rows represent the estimated classes. The values within the matrix correspond to the number of items that fall into every cell. 
The marginals of the columns (true classes) represent the percentage of misclassified items in relation to the 'truth', commonly known as the \textit{omission error}. And the marginals of the rows (estimated classes) represent the percentage of misclassified items in relation to the estimates (classifier), commonly known as the \textit{commission error}. The percentage in the down right cell of the matrix is the overall mean misclassification rate. As can be seen, there exist a high misclassification rate between classes $k=1$ and $k=2$, between classes $k=1$ and $k=3$, and between classes $k=1$ and $k=5$.

\begin{table}
\begin{center}
\begin{tabular}{|c|*{6}{c|}}\hline
\backslashbox{\small Estimate}{\small True} & \multicolumn{1}{p{1cm}|}{\centering k = 1} &  \multicolumn{1}{p{1cm}|}{\centering k = 2} & \multicolumn{1}{p{1cm}|}{\centering k = 3} & \multicolumn{1}{p{1cm}|}{\centering k = 4} & \multicolumn{1}{p{1cm}|}{\centering k = 5} & \multicolumn{1}{p{1cm}|}{\centering } \\ 
\hline \multicolumn{1}{|p{3cm}|}{ \centering k = 1} &90&39&14&3&11&42\%\\
\hline \multicolumn{1}{|p{3cm}|}{\centering k = 2} &46&301&8&2&3&16\%\\
\hline \multicolumn{1}{|p{3cm}|}{\centering k = 3}  &8&4&59&4&1&19\%\\
\hline \multicolumn{1}{|p{3cm}|}{\centering k = 4}  &5&2&6&342&5&5\%\\
\hline \multicolumn{1}{|p{3cm}|}{\centering k = 5}  &8&2&1&0&38&19\%\\
\hline \multicolumn{1}{|p{3cm}|}{\centering }  &42\%&13\%&32\%&2\%&34\%&$\textbf{17\%}$\\
\hline
\end{tabular}
\end{center}
\caption{Confusion matrix after the Q-fold cross-validation procedure over the training data.}
  \label{ch5_tab_parcels}
\end{table}

The Stan model code for the HSGP model of this case study can be found online at {\small \url{https://github.com/gabriuma/basis_functions_approach_to_GP/tree/master/Paper/Case-study_Land-use-classification}}\,.


\bibliography{../Manuscript_Stats&Comp/references}

\end{document}


